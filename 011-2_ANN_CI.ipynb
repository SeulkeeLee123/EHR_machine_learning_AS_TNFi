{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import interp\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.utils import resample\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "from keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# You have to modify file names and path to files as you need.\n",
    "path_to_data_file = 'PATH/TO/DATA/FILE'\n",
    "test_index_file = 'PATH/TO/TEST/INDEX'\n",
    "train_index_file = 'PATH/TO/TRAIN/INDEX'\n",
    "saved_model_folder = 'PATH/TO/MODEL/SAVED'\n",
    "performance_file = 'PATH/TO/PERFORMANCE/FILE'\n",
    "n_seed = 1 # Insert the seed of your best model\n",
    "##################\n",
    "\n",
    "whole_data = [line.strip().split('\\t') for line in open(path_to_data_file)]\n",
    "\n",
    "del(whole_data[0])\n",
    "\n",
    "featDic = {}\n",
    "data_list_x = []\n",
    "data_list_y = []\n",
    "\n",
    "for line in whole_data:\n",
    "    featDic[line[0]] = list(map(float,line[1:11])) + list(map(int,line[11:13])) + list(map(float,line[13:15])) + list(map(int,line[15]))\n",
    "    data_list_x.append(featDic[line[0]][:-1])\n",
    "    data_list_y.append(featDic[line[0]][-1])\n",
    "    \n",
    "data_x_bf_st = np.array(data_list_x)\n",
    "data_x = (data_x_bf_st - np.mean(data_x_bf_st, axis=0))/np.std(data_x_bf_st, axis=0)\n",
    "data_y = np.array(data_list_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = n_seed\n",
    "\n",
    "dict_train = {} # key = model name, value = index\n",
    "dict_test = {}  # key = model name, value = index\n",
    "train_index_data = [line.strip().split('\\t') for line in open(train_index_file)]\n",
    "test_index_data = [line.strip().split('\\t') for line in open(test_index_file)]\n",
    "\n",
    "for line in train_index_data:\n",
    "    dict_train[line[0]] = [int(x) for x in line[1].split(',')]\n",
    "for line in test_index_data:\n",
    "    dict_test[line[0]] = [int(x) for x in line[1].split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "n_iterations = 1000\n",
    "\n",
    "for k in range(1,4):\n",
    "\n",
    "    train = dict_train['%i_%i' % (seed, k)]\n",
    "    test = dict_test['%i_%i' % (seed, k)]\n",
    "        \n",
    "    model = keras.models.load_model(saved_model_folder + '/%i_%i_noca.h5' % (seed,k))\n",
    "    \n",
    "    n_size_train = len(data_x[train])\n",
    "    n_size_test = len(data_y[test])\n",
    "    \n",
    "    stats = defaultdict(list)\n",
    "    for i in range(n_iterations):\n",
    "        train_bootstrap_x, train_bootstrap_y = resample(data_x[train], data_y[train], n_samples=n_size_train)\n",
    "        test_bootstrap_x, test_bootstrap_y = resample(data_x[test], data_y[test], n_samples=n_size_test)\n",
    "        \n",
    "        _ , acc_b = model.evaluate(test_bootstrap_x, test_bootstrap_y)\n",
    "        predictions_bootstrap = model.predict(test_bootstrap_x)\n",
    "        \n",
    "        fpr_b, tpr_b, _ = metrics.roc_curve(test_bootstrap_y, predictions_bootstrap)\n",
    "        roc_b = metrics.auc(fpr_b, tpr_b)\n",
    "        \n",
    "        precision_b, recall_b, _ = precision_recall_curve(test_bootstrap_y, predictions_bootstrap)\n",
    "        f1_b = f1_score(test_bootstrap_y, predictions_bootstrap.round())\n",
    "        \n",
    "        rp_b = metrics.auc(recall_b, precision_b)\n",
    "        \n",
    "        stats['acc'].append(acc_b)\n",
    "        stats['roc'].append(roc_b)\n",
    "        stats['f1'].append(f1_b)\n",
    "        stats['rp'].append(rp_b)\n",
    "    \n",
    "    alpha=0.95\n",
    "    p_l = ((1.0-alpha)/2) * 100\n",
    "    p_u = (alpha + ((1.0 - alpha)/ 2.0)) * 100\n",
    "\n",
    "    mean_perf = dict()\n",
    "    lower = dict()\n",
    "    upper = dict()\n",
    "\n",
    "    with open(performance_file, 'a') as result:\n",
    "        perf_list = ['acc', 'roc', 'f1', 'rp']\n",
    "        for perf in perf_list:\n",
    "            mean_perf[perf] = sum(stats[perf]) / len(stats[perf])\n",
    "            lower[perf] = max(0.0, np.percentile(stats[perf],p_l))\n",
    "            upper[perf] = min(1.0, np.percentile(stats[perf],p_u))\n",
    "            result.write('\\t'.join([perf, str(mean_perf[perf]), str(lower[perf]), str(upper[perf]), str(k)]) + '\\n')\n",
    "        \n",
    "    k = k + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

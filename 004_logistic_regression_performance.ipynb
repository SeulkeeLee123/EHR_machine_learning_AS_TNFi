{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# You can modify file names and path to files as you need.\n",
    "path_to_data_file = 'data_file.txt'\n",
    "performance_file = 'logistic_hyperparameter_tuning.txt'\n",
    "##################\n",
    "\n",
    "iteration = int(sys.argv[1])\n",
    "\n",
    "whole_data = [line.strip().split('\\t') for line in open(path_to_data_file)]\n",
    "\n",
    "del(whole_data[0])\n",
    "\n",
    "featDic = {}\n",
    "data_list_x = []\n",
    "data_list_y = []\n",
    "\n",
    "for line in whole_data:\n",
    "    featDic[line[0]] = list(map(float,line[1:11])) + list(map(int,line[11:13])) + list(map(float,line[13:15])) + list(map(int,line[15]))\n",
    "    data_list_x.append(featDic[line[0]][:-1])\n",
    "    data_list_y.append(featDic[line[0]][-1])\n",
    "    \n",
    "data_x = np.array(data_list_x)\n",
    "data_y = np.array(data_list_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_1 = 1209 # random number\n",
    "np.random.seed(seed_1)\n",
    "n_seed = np.random.randint(10000)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=n_seed)\n",
    "\n",
    "accs = []\n",
    "ROC_aucs = []\n",
    "f1s = []\n",
    "RP_aucs = []\n",
    "\n",
    "for train, test in cv.split(data_x, data_y):\n",
    "    \n",
    "    clf = LogisticRegression(random_state=123).fit(data_x[train], data_y[train])\n",
    "\n",
    "    test_acc = clf.score(data_x[test], data_y[test])\n",
    "    \n",
    "    predictions = clf.predict_proba(data_x[test])[:,1]\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(data_y[test], predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    accs.append(test_acc)\n",
    "    ROC_aucs.append(roc_auc)\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(data_y[test], predictions)\n",
    "    f1 = f1_score(data_y[test], predictions.round())\n",
    "    f1s.append(f1)\n",
    "        \n",
    "    rp_auc = metrics.auc(recall, precision)\n",
    "    RP_aucs.append(rp_auc)\n",
    "    \n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = metrics.auc(mean_fpr, mean_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(performance_file, 'a') as perf_file:\n",
    "    perf_file.write('\\t'.join([str(n_seed), str(iteration), str(sum(accs) / 3), '\\t'.join(str(x) for x in accs), \n",
    "                               str(mean_auc), '\\t'.join(str(x) for x in ROC_aucs),\n",
    "                               str(sum(f1s)/3), '\\t'.join(str(x) for x in f1s),\n",
    "                               str(sum(RP_aucs) / 3), '\\t'.join(str(x) for x in RP_aucs)]) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
